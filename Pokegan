from __future__ import print_function
%matplotlib inline
import argparse

import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML
from torch.autograd import Variable
import math

# Взводим руками рандом сид для воспроизводимости обучения
manualSeed = 333
print("Random Seed: ", manualSeed)
random.seed(manualSeed)
torch.manual_seed(manualSeed)

dataroot = "./pokemon/"

workers = 1

batch_size = 64

nz = 100

num_epochs = 1000

# Создаем даталоадер
image_size = 64
dataset = dset.ImageFolder(root=dataroot,
                           transform=transforms.Compose([
                               transforms.Resize(image_size),
                               transforms.CenterCrop(image_size),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                           ]))

dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                         shuffle=True, num_workers=workers)

# переключаемся на куду
device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")

print("Current device : {}".format(device))

# Выведем трейнсет, дабы убедиться, что все завелось
real_batch = next(iter(dataloader))
plt.figure(figsize=(8,8))
plt.axis("off")
plt.title("Training Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))

# функция для инициализации весов из статьи про GAN
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
        
# Генератор

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.p=0.1
        self.features = nn.Sequential()
        self.features.add_module('convT1',nn.ConvTranspose2d(100, 1024, 4, 1))
        self.features.add_module('batch1',nn.BatchNorm2d(1024))
        self.features.add_module('relu1', nn.ReLU())
        self.features.add_module('convT2',nn.ConvTranspose2d(1024, 512, 4, 2, 1))
        self.features.add_module('batch2',nn.BatchNorm2d(512))
        self.features.add_module('relu2', nn.ReLU())
        self.features.add_module('convT3',nn.ConvTranspose2d(512, 256, 4, 2, 1))
        self.features.add_module('batch3', nn.BatchNorm2d(256))
        self.features.add_module('relu3', nn.ReLU())
        self.features.add_module('convT4',nn.ConvTranspose2d(256, 128, 4, 2, 1))
        self.features.add_module('batch4',nn.BatchNorm2d(128))
        self.features.add_module('relu4', nn.ReLU())
        self.features.add_module('convT5',nn.ConvTranspose2d(128, 3, 4, 2, 1))
        self.features.add_module('tanh',nn.Tanh())
    def forward(self, input):
        x=self.features(input)
        return x
 # Создали генератор
netG = Generator().to(device)

# Инициализировали его
netG.apply(weights_init)

# Вывели модель для отладки
print(netG)

# Дискриминатор
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.p=0.1
        self.features = nn.Sequential()
        self.features.add_module('conv1',nn.Conv2d(3, 128, 4, 2, 1))
        self.features.add_module('batch4',nn.BatchNorm2d(128))
        self.features.add_module('lrelu1',nn.LeakyReLU(0.2))
        self.features.add_module('drop1',nn.Dropout(self.p))
        self.features.add_module('conv2', nn.Conv2d(128, 256, 4, 2, 1))
        self.features.add_module('batch1',nn.BatchNorm2d(256))
        self.features.add_module('lrelu2',nn.LeakyReLU(0.2))
        self.features.add_module('drop2',nn.Dropout(self.p))
        self.features.add_module('conv3',nn.Conv2d(256, 512, 4, 2, 1))
        self.features.add_module('batch2', nn.BatchNorm2d(512))
        self.features.add_module('lrelu3', nn.LeakyReLU(0.2))
        self.features.add_module('drop3',nn.Dropout(self.p))
        self.features.add_module('conv4',nn.Conv2d(512, 1024, 4, 2, 1))
        self.features.add_module('batch3',nn.BatchNorm2d(1024))
        self.features.add_module('relu4',  nn.LeakyReLU(0.2))
        self.features.add_module('drop4',nn.Dropout(self.p))
        self.features.add_module('conv5',nn.Conv2d(1024, 1, 4, 1))
        self.features.add_module('sigm',nn.Sigmoid())
    def forward(self, input):
        x=self.features(input)
        self.p=0.1*math.log(x.mean()/0.1)
        return x.view(-1, 1).squeeze(1)
 # Создали дискриминатор
netD = Discriminator().to(device)

# Ининциализировали весе
netD.apply(weights_init)

# Вывели модель для отладки
print(netD)

# Фиксированный шум для мониторинга качества генерации
fixed_noise = torch.randn(64, nz, 1, 1, device=device)

# Коэффициент обучения
lr = 0.00005

# Оптимизаторы для сеток
optimizerD = optim.RMSprop(netD.parameters(), lr=lr)
optimizerG = optim.RMSprop(netG.parameters(), lr=lr)

def train_D(discriminator, images, fake_images):
            discriminator.zero_grad()
            outputs = discriminator(images)
            real_loss = torch.mean(outputs)
            real_score = outputs
            outputs=discriminator(fake_images)
            fake_loss = torch.mean(outputs)
            fake_score = outputs
            loss = -((real_loss) - (fake_loss))
            loss.backward()
            optimizerD.step()
            
            for i in discriminator.parameters():
                i.data.clamp_(-0.01, 0.01)
            return loss, real_score, fake_score
            
    # код для обучения генератора
def train_G(generator, discriminator, fake_images):
            generator.zero_grad()
            loss =  -(torch.mean(discriminator(generator(fake_images))))
            loss.backward()
            optimizerG.step()
            return loss
   # Training Loop

# Lists to keep track of progress
img_list = []
G_losses = []
D_losses = []
iters = 0

d_episodes = 1

print("Starting Training Loop...")
for epoch in range(num_epochs):

    data_iter = iter(dataloader)
    i = 0
    
    while i < len(dataloader):
        img = next(data_iter)[0].to(device)
        img = Variable(img)

        for _ in range(5):
            noise = Variable(torch.randn(64, 100, 1, 1)).to(device)
            fake_images = netG(noise).to(device)
            d_loss, real_score, fake_score = train_D(netD, img, fake_images)
        noise = Variable(torch.randn(64, 100, 1, 1)).to(device)
        g_loss = train_G(netG,netD, noise)
        
        # обучаем дискриминатор
        # d_loss = wasserstein loss дискриминатора
        # real_score - оценки дискриминатора для настоящих картинок
        # fake_score - оценки дискриминатора для фейковых картинок
        # помните, что на одну итерацию обучения генератора
        # должно приходиться несколько итераций обучения дискриминатора!
        
        # обучаем генератор
        # g_loss = critic-loss генератора
        i += 1
        # Выводим стату
        if i % 1 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f'
                  % (epoch, num_epochs, i, len(dataloader),
                     d_loss.item(), g_loss.item(), real_score.mean().item(), fake_score.mean().item()))

        # Save Losses for plotting later
        G_losses.append(g_loss.item())
        D_losses.append(d_loss.item())

        # Сохраняем результат генерации для нашего фиксированного шума( для валидации )
        if (iters % 50 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
            with torch.no_grad():
                fake = netG(fixed_noise).detach().cpu()
            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

        iters += 1
 
3# Выводим на печать всяческие loss'ы

plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses,label="G")
plt.plot(D_losses,label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()
